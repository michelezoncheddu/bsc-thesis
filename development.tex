\chapter{Development}
This chapter will cover the new implementations and changes to the project's software, illustrating the improvement of previous features, the new ones and the way in which they have been achieved.
\bigbreak

In the previous project's code, some time and memory optimizations have been made, minor bugs have been found and fixed and some other improvements have been made (as replacing deprecated methods and libraries with newer ones).

\section{MQTT QoS2 caching}
As mentioned in the \textit{Unsolved Issues} chapter of Dario Piotrowicz's thesis \cite{Pio19}, there was a problem with unreliable Wi-Fi networks (an issue in the library's GitHub repository is still open \cite{githubQos2Issue}): all messages generated while a broken client-broker connection were not stored to be sent when the connection is re-established.

To solve that issue have been implemented two queues, one for each connection: the playing session one and the neural network's pattern collection one. The Particle Photon is equipped with 128 kilobytes of RAM, but while the program is running the lowest value of available memory drops down to almost 32 kilobytes (bigger values led the Photon out of memory). The maximum cache size is calculated and stored in the \texttt{cache\_size} variable. With a maximum message size of 512 characters (511 effective characters because of the null terminated string), the cache size for each queue is easily calculated as 32.

Follows the playing session's client caching implementation; the neural network one is analogous.
\bigbreak

\begin{lstlisting}[style=CPPStyle]

	...

	// ready to send the data (publishString) to the client
	if (client.isConnected()) {
        while (cached_messages.size() > 0) {
			// send cached messages
            std::string msg = cached_messages.front();
            cached_messages.pop();
			client.publish("motiontracker/" + ID,
							msg.c_str(),
							MQTT::QOS2);
        }
		client.publish("motiontracker/" + ID,
						publishString,
						MQTT::QOS2);
    } else if (cached_messages.size() < cache_bound) {
		// save message in cache
        std::string msg(publishString);
        cached_messages.push(msg);
    }

	...

\end{lstlisting}
\bigbreak

This solution has made possible to damper temporary disconnections caused by instable connections.

\section{Motion capture}

\begin{center}
	\begin{figure}[ht!]
		\makebox[\textwidth]{\includegraphics[width=0.6\paperwidth]{img/data_fusion.png}}
		\caption{Data fusion schema.}
	\end{figure}
\end{center}
Previously, the data collected were:
\begin{itemize}
	\item yaw, pitch and roll angles;
	\item acceleration with components in spherical coordinates system;
	\item raw gyroscope data;
	\item a velocity approximation.
\end{itemize}
\bigbreak

The aim was to detect \textit{discriminating features among different playing patterns}, collecting and analyzing data from the sensors; the analysis followed the flow in the data fusion schema, except for the velocity approximation.
\bigbreak

The velocity approximation, calculated by the \textit{Velocimeter} \cite{Pio19} was excluded because of its practical difficulty in obtaining realistic values using only inertial sensors, where measurement errors are unavoidable, especially for miniature MEMS sensors. Direct integration of acceleration often causes unrealistic drifts in velocity, due to errors propagation; furthermore, measured acceleration not only carries random noise, but also presents with offset caused by temperature drift, resulting in estimation errors accumulated by integration process \cite{Du15, Est14, Kow15, Liu01, Sei07, UsingAcc, Woo07, Yan06}, that results in low accuracy. Were particularly problematic stillness detection after a movement, that resulted in a peak toward zero instead of the real decrease, and repetitive light accelerations, like the one generated by the rolling of the tessellated tyres, that resulted in non-present velocities readings in some axes.
\bigbreak

\begin{center}
	\begin{figure}[ht!]
		\makebox[\textwidth]{\includegraphics[width=0.2\paperwidth]{img/plots/square.png}}
		\caption{Square wave and non present accelerations.}
	\end{figure}
\end{center}

The first analysis dealt with raw accelerometer data, but the noisiness of the signal and the lack of gravity removal have induced to desist. Nevertheless, more often than not the main waveform was recognizable, especially for movements with strong accelerations (that simply increase the signal-to-noise ratio).
\bigbreak

The analysis moved to Kalman-filtered data, which revealed much cleaner, but still included gravity; thus it was necessary a more complex training data collection, due to the lack of orientation invariance (in order to obtain such invariance it would have been necessary to collect every pattern with the toy rotated in any possible angle).

Nevertheless, gravity-free acceleration data were computed by the device (with the help of Madgwick's fusion algorithm) and sent to the server but not stored in the database, only used for the 3D visual representation, both in training and session recording phase. It was decided to keep such data and analyze it to see if it was reliable over time.
\bigbreak

Some testing showed quickly that there was an error in the yaw angle calculation: the more the toy was rotated (regardless of the axis), the more the yaw diverged. Such error was initially imputed to the Madgwick algorithm implementation, more precisely to the convergence of its error-correction gradient descent algorithm. Unfortunately, different algorithm implementations and different values of the algorithm's gain parameter (the magnitude $\beta$ of the gyroscope measurement error\footnote{Increasing $\beta$ leads to faster bias corrections and higher sensitiveness to lateral accelerations.} \cite[13]{Mad10}) have no given better results over time, and worse, sometimes ghost accelerations were found.
\bigbreak

\begin{center}
	\begin{figure}[ht!]
		\makebox[\textwidth]{\includegraphics[width=0.6\paperwidth]{img/plots/drift.png}}
		\caption{Different accelerations for the same pattern.}
	\end{figure}
\end{center}

During the tests, it was noticed that recordings of the same pattern with different device orientations didn't split the acceleration differently among axes, revealing that the reference system was integral with the Earth's center instead of the device's sensor center. To align it back, the acceleration vector has been rotated using a 3D rotation matrix. The 3 rotation angles are given by the Madgwick algorithm.
\bigbreak

A \textit{yaw} is a CCW rotation of $\alpha$ on the $z$-axis. The rotation matrix is
\[
	R_z(\alpha) =
	\begin{pmatrix}
		\cos\alpha & -\sin\alpha & 0 \\
		\sin\alpha & \cos\alpha & 0 \\
		0 & 0 & 1
	\end{pmatrix}
\]
Note that the upper left values compose a 2D rotation matrix, and the coordinates on the third dimension (around whom the rotation happens) are left unchanged. The same applies to the other two matrices, but with the second and the first dimension.
\bigbreak

A \textit{pitch} is a CCW rotation of $\beta$ on the $y$-axis. The rotation matrix is
\[
	R_y(\beta) =
	\begin{pmatrix}
		\cos\beta & 0 & \sin\beta \\
		0 & 1 & 0 \\
		-sin\beta & 0 & \cos\beta
	\end{pmatrix}
\]

A \textit{roll} is a CCW rotation of $\gamma$ on the $x$-axis. The rotation matrix is
\[
	R_x(\gamma) =
	\begin{pmatrix}
		1 & 0 & 0 \\
		0 & \cos\gamma & -\sin\gamma \\
		0 & sin\gamma & \cos\gamma
	\end{pmatrix}
\]
\begin{gather*}
	R(\alpha, \beta, \gamma) = R_z(\alpha) R_y(\beta) R_x(\gamma) = \\
	\begin{pmatrix}
		\cos\alpha \cos\beta & \cos\alpha \sin\beta \sin\gamma - \sin\alpha \cos\gamma & \cos\alpha \sin\beta \cos\gamma + \sin\alpha \sin\gamma \\
		\sin\alpha \cos\beta & \sin\alpha \sin\beta \sin\gamma + \cos\alpha \cos\gamma & \sin\alpha \sin\beta \cos\gamma - \cos\alpha \sin\gamma \\
		-\sin\beta & \cos\beta \sin\gamma & \cos\beta \cos\gamma
	\end{pmatrix}
\end{gather*}
It is important to note that $R(\alpha, \beta, \gamma)$ performs the roll first, then the pitch, and finally the yaw. If the order of these operations is changed, a different rotation matrix would result \cite{Lav06}.
\bigbreak

Given an acceleration vector $\vec a$, the rotated one is $\vec a_r = R(\alpha, \beta, \gamma) \vec a$.
The matrix-vector multiplication doesn't need specific optimizations in the code because the dimensions at stake are too small for a meaningful speed or space improvement; the frequency of packets sent (22 Hertz) has been always achieved (when the wireless connection allowed it) and however the C++ code is compiled by \texttt{GCC} with the \texttt{-Os} flag, that includes the loop unrolling optimization \cite{UsingGCC}.
\bigbreak

The coordinate system rotation solved both problems: the coordinate system was integral with the toy, and the yaw angle was calculated correctly. Because of this rotation, the accelerations shown in the 3D visualization may diverge from the reality after long playing sessions, but the error involves only the representation, not the real data. However, the 3D visualization tool will be probably removed in future works.
\bigbreak

\begin{center}
	\begin{figure}[ht!]
		\makebox[\textwidth]{\includegraphics[width=0.6\paperwidth]{img/plots/nodrift.png}}
		\caption{Image of PERFECT data.}
	\end{figure}
\end{center}

Once achieved a reliable gravity-filtered acceleration approximation, it was possible to perform some fine tuning. Magnetic and inertial sensors can be influenced by magnetic disturbance \cite{Fan17}. Madgwick's algorithm can remove interferences from Earth's frame (\textit{soft iron}) \cite[11-12]{Mad10} that cause errors in the measured direction of the Earth's magnetic field. \textit{Hard iron} distortions are produced by materials that exhibit a constant, additive field to the Earth's magnetic field \cite{CompensatingIron}, and they can be removed through calibration \cite{CompensatingIron, Geb06, Kok12} [CITE from 42 to 45 in Madgwick's report]. The calibration has been performed by the SparkFun's Arduino library.

Unfortunately, there are non-constant interferences that still affect the sensors readings, such as the magnetic field generated by the current passing through the USB cable for charging the battery.
Its removal is much harder, because requires to analyze how much current is flowing through the cable, and the knowledge of the net magnetic field's shape, that depends on how the cable is placed around the sensor.
\bigbreak

\begin{center}
	\begin{figure}[ht!]
		\makebox[\textwidth]{\includegraphics[width=0.6\paperwidth]{img/plots/battery.png}}
		\caption{Orientation affected by the battery charging.}
	\end{figure}
\end{center}

\section{Delay in real-time data plotting}
\dots
